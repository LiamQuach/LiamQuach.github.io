{
  "hash": "6b81b8c33be5fc2213f6bba5dcaf6ad5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 8: Web Scrapping\"\ndescription: \"Cheese Gromit!\"\nimage: image.png\nauthor:\n  - name: Liam Quach\n    url: https://LiamQuach.github.io/\n    affiliation: Master of Science in Statistics @ Cal Poly\ndate: 05-04-2025\ncategories: [Quarto, R] # self-defined categories\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nexecute: \n  echo: true\n  warning: false\n---\n\n\n\n> **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤\n\n**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize what you learn from it.\n\nThe robots.txt is located at <https://www.cheese.com/robots.txt> . This file is pretty blank, with only two lines. These two lines tell us that anyone can crawl (`User-agent` is set to \\*) and where the site map is located (<https://www.cheese.com/sitemap.xml>)\n\n**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how this function works with a small example.\n\nThe `html_attr()` function gets and returns a character vector containing the values of the specified attribute for an element of the HTML file (ex. href, class, id). This function requires two arguments; `x`, the HTML element (usually from `read_html()`), `name`, the name of the attribute to retrieve, like `href`, `class`, etc. There is a third optional argument, `default`, which is related to how non-existent attributes are handled, which defaults to `NA_character`, meaning NA is returned in these cases\n\n\n**Part 3:** (Do this alongside Part 4 below.) I used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:\n\n> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.\n\nFully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(rvest)\nlibrary(dplyr)\n# Define the URL\nurl <- \"https://www.cheese.com/alphabetical\"\n\n# Read the HTML content from the webpage\nwebpage <- read_html(url)\n\n# Extract the cheese names and URLs\ncheese_data <- webpage %>%\n  html_nodes(\".cheese-item\") %>%\n  html_nodes(\"a\") %>%\n  html_attr(\"href\") %>%\n  paste0(\"https://cheese.com\", .)\n\ncheese_names <- webpage %>%\n  html_nodes(\".cheese-item h3\") %>%\n  html_text()\n\n# Create a data frame to store the results\ncheese_df <- data.frame(Name = cheese_names,\n                        URL = cheese_data,\n                        stringsAsFactors = FALSE)\n\n# Print the data frame\nprint(cheese_df)\n```\n:::\n\n\n\nEmpty Results: Both cheese_data (for URLs) and cheese_names (for cheese names) were empty character vectors. Consequently, the cheese_df dataframe was also empty.\n\nSelector Specificity: The CSS selectors .cheese-item and .cheese-item h3 suggested by ChatGPT were likely too generic or did not accurately reflect the current structure of the cheese.com/alphabetical page. Websites frequently update their structure, and AI models might be trained on outdated versions or make incorrect assumptions about common class names.\n\nUsefulness of ChatGPT: In this instance, ChatGPT provided a basic template of rvest functions (read_html, html_nodes, html_attr, html_text) which was conceptually helpful for recalling the workflow. \n\n\n**Part 4:** Obtain the following information for **all** cheeses in the database:\n\n-   cheese name\n-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)\n-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).\n\nTo be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\n\nbase_url <- \"https://www.cheese.com/alphabetical/?per_page=100\"\npage_numbers <- 1:21\n\n# Helper function to extract text or attributes based on tag structure\nextract_info <- function(page, outer_selector, inner_selector, attr = NULL) {\n  nodes <- page %>%\n    html_elements(outer_selector) %>%\n    html_elements(inner_selector)\n  \n  if (!is.null(attr)) {\n    html_attr(nodes, attr)\n  } else {\n    html_text(nodes)\n  }\n}\n\n# Function to scrape a single page\nscrape_cheese_page <- function(page_number) {\n  full_url <- paste0(base_url, \"&page=\", page_number)\n  page <- read_html(full_url)\n\n  data.frame(\n    Name = extract_info(page, \"div.product-item\", \"h3\"),\n    url = paste0(\"https://www.cheese.com\", extract_info(page, \"div.product-item\", \"h3 a\", \"href\")),\n    whether = extract_info(page, \"div.product-item\", \"img\", \"class\"),\n    stringsAsFactors = FALSE\n  )\n}\n\n# Map over all pages and bind results\ncheese_data <- map_dfr(page_numbers, function(pg) {\n  result <- scrape_cheese_page(pg)\n  Sys.sleep(1)  # delay \n  result\n})\n\nhead(cheese_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                               Name\n1           2 Year Aged Cumin Gouda\n2            3-Cheese Italian Blend\n3 30 Month Aged Parmigiano Reggiano\n4           3yrs Aged Vintage Gouda\n5                        Aarewasser\n6                  Abbaye de Belloc\n                                                             url       whether\n1                https://www.cheese.com/2-year-aged-cumin-gouda/  image-exists\n2                 https://www.cheese.com/3-cheese-italian-blend/ image-missing\n3 https://www.cheese.com/30-month-aged-parmigiano-reggiano-150g/  image-exists\n4                https://www.cheese.com/3yrs-aged-vintage-gouda/  image-exists\n5                             https://www.cheese.com/aarewasser/  image-exists\n6                       https://www.cheese.com/abbaye-de-belloc/  image-exists\n```\n\n\n:::\n:::\n\n\n\n**Part 5:** When you go to a particular cheese's page (like [gouda](https://www.cheese.com/gouda/)), you'll see more detailed information about the cheese. For [**just 10**]{.underline} of the cheeses in the database, obtain the following detailed information:\n\n-   milk information\n-   country of origin\n-   family\n-   type\n-   flavour\n\n(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_text <- function(page, selector) {\n  page %>%\n    html_elements(selector) %>%\n    html_text()\n}\n\n# Scrape cheese detail from a single page URL\nscrape_cheese_details <- function(url) {\n  Sys.sleep(1)  # delay\n  \n  page <- read_html(url)\n  \n  tibble(\n    milk = extract_text(page, \".summary_milk p\"),\n    country_of_origin = extract_text(page, \".summary_country p\"),\n    family = extract_text(page, \".summary_family p\"),\n    type = extract_text(page, \".summary_moisture_and_type p\"),\n    flavour = extract_text(page, \".summary_taste p\")\n  )\n}\n\n# Select cheese URLs of interest\ncheese_urls <- cheese_data %>%\n  filter(Name %in% c(\n    \"Amul Processed Cheese\", \"Cheese Curds\", \"Colby\", \"Applewood\",\n    \"Vacherin\", \"Mozzarella di Bufala DOP\", \"Pecorino Romano\",\n    \"Paesanella Burrata\", \"Camembert\", \"Paesanella Cherry Bocconcini\"\n  )) %>%\n  pull(url)\n\n# Map and combine all details into a single tibble\ndf_cheeses <- map_dfr(cheese_urls, scrape_cheese_details)\n\nhead(df_cheeses)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 5\n  milk                                    country_of_origin family type  flavour\n  <chr>                                   <chr>             <chr>  <chr> <chr>  \n1 Made from pasteurized cow's or water bâ€¦ Country of origiâ€¦ Familâ€¦ Typeâ€¦ \"Flavoâ€¦\n2 Made from pasteurized cow's milk        Country of origiâ€¦ Familâ€¦ Typeâ€¦ \"Flavoâ€¦\n3 Made from cow's milk                    Country of origiâ€¦ Familâ€¦ Typeâ€¦ \"Flavoâ€¦\n4 Made from cow's milk                    Country of origiâ€¦ Familâ€¦ Typeâ€¦ \"Flavoâ€¦\n5 Made from pasteurized water buffalo's â€¦ Country of origiâ€¦ Familâ€¦ Typeâ€¦ \"Flavoâ€¦\n6 Made from pasteurized cow's milk        Country of origiâ€¦ Familâ€¦ Typeâ€¦ \"Flavoâ€¦\n```\n\n\n:::\n:::\n\n\n\n**Part 6:** Evaluate the code that you wrote in terms of **efficiency**. To what extent do your function(s) adhere to the **principles for writing good functions**? To what extent are your **functions efficient**? To what extent is your **iteration of these functions efficient**?\n\nThe functions we wrote follow the principles of good function design by being modular, clear, and reusable. Each function performs a single responsibility. For example, `extract_info()` and `scrape_cheese_details()` are each focused on one task. Using `purrr::map_dfr()` improves efficiency by combining iteration and row-binding in a memory-friendly way, avoiding repeated `rbind()` calls that can slow down execution. While `Sys.sleep(1)` adds intentional delay, itâ€™s necessary for responsible scraping. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}